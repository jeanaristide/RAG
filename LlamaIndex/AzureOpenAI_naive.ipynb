{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/jeana/.env')\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.WARNING\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "azure_endpoint = os.environ['OPENAI_DEPLOYMENT_ENDPOINT']\n",
    "api_version = os.environ['OPENAI_DEPLOYMENT_VERSION']\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model= os.environ['OPENAI_MODEL_NAME'],\n",
    "    deployment_name= os.environ['OPENAI_DEPLOYMENT_NAME'],\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=os.environ['OPENAI_EMBEDDING_MODEL_NAME'],\n",
    "    deployment_name=os.environ['OPENAI_EMBEDDING_DEPLOYMENT_NAME'],\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import set_global_service_context\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if storage already exists\n",
    "if not os.path.exists(\"./storage/\"+os.environ['OPENAI_EMBEDDING_MODEL_NAME']):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(input_files=[r\"/Users/jeana/Retrieval-Augmented-Generation/LlamaIndex/paul_graham_essay.txt\"] #or just indicate the fullpath of the folder containing the data\n",
    "                    ).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(\"./storage/\"+os.environ['OPENAI_EMBEDDING_MODEL_NAME'])\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage/\"+os.environ['OPENAI_EMBEDDING_MODEL_NAME'])\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[r\"/Users/jeana/Retrieval-Augmented-Generation/LlamaIndex/paul_graham_essay.txt\",\n",
    "                 r\"/Users/jeana/Retrieval-Augmented-Generation/2311.12399.pdf\"\n",
    "                 ] \n",
    "    #or just indicate the fullpath of the folder containing the data\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 05682d95-1ee9-43b2-ac3d-fa1493f2f5a3): A Survey of Graph Meets Large Language Model: Progress and Future Directions\n",
      "Yuhan Li1âˆ—,Zhixun Li...\n",
      "\n",
      "> Source (Doc id: 7336d56a-9e08-43d4-a405-fd666a131a71): ,\n",
      "2023d]. While their primary focus has been on text sequences,\n",
      "there is a growing interest in en...\n",
      "query was: How does LLM help graph-related tasks?\n",
      "answer was: LLMs help graph-related tasks by enhancing the way we interact with graphs, particularly those containing nodes associated with text attributes. The integration of LLMs with graphs has demonstrated success in various downstream tasks across different graph domains. By combining LLMs with traditional GNNs, graph learning can be enhanced. LLMs provide stronger node features that capture both structural and contextual aspects, while GNNs excel at capturing structural information. This combination allows for a more comprehensive understanding of graph data, leveraging the robust textual understanding of LLMs and the structural information captured by GNNs.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LLM help graph-related tasks?\"\n",
    "query_engine = index.as_query_engine()\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(answer.get_formatted_sources())\n",
    "print(\"query was:\", query)\n",
    "print(\"answer was:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
