{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/jeana/.env')\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.WARNING\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "azure_endpoint = os.environ['OPENAI_DEPLOYMENT_ENDPOINT']\n",
    "api_version = os.environ['OPENAI_DEPLOYMENT_VERSION']\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model= os.environ['OPENAI_MODEL_NAME'],\n",
    "    deployment_name= os.environ['OPENAI_DEPLOYMENT_NAME'],\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=os.environ['OPENAI_EMBEDDING_MODEL_NAME'],\n",
    "    deployment_name=os.environ['OPENAI_EMBEDDING_DEPLOYMENT_NAME'],\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import set_global_service_context\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if storage already exists\n",
    "if not os.path.exists(\"./storage/\"+os.environ['OPENAI_EMBEDDING_MODEL_NAME']):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(input_files=[r\"/Users/jeana/Retrieval-Augmented-Generation/LlamaIndex/paul_graham_essay.txt\"] #or just indicate the fullpath of the folder containing the data\n",
    "                    ).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(\"./storage/\"+os.environ['OPENAI_EMBEDDING_MODEL_NAME'])\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage/\"+os.environ['OPENAI_EMBEDDING_MODEL_NAME'])\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[r\"/Users/jeana/Retrieval-Augmented-Generation/LlamaIndex/paul_graham_essay.txt\",\n",
    "                 r\"/Users/jeana/Retrieval-Augmented-Generation/2311.12399.pdf\"\n",
    "                 ] \n",
    "    #or just indicate the fullpath of the folder containing the data\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 05682d95-1ee9-43b2-ac3d-fa1493f2f5a3): A Survey of Graph Meets Large Language Model: Progress and Future Directions\n",
      "Yuhan Li1∗,Zhixun Li...\n",
      "\n",
      "> Source (Doc id: 7336d56a-9e08-43d4-a405-fd666a131a71): ,\n",
      "2023d]. While their primary focus has been on text sequences,\n",
      "there is a growing interest in en...\n",
      "query was: How does LLM help graph-related tasks?\n",
      "answer was: LLMs help graph-related tasks by enhancing the way we interact with graphs, particularly those containing nodes associated with text attributes. The integration of LLMs with graphs has demonstrated success in various downstream tasks across different graph domains. By combining LLMs with traditional GNNs, graph learning can be enhanced. LLMs provide stronger node features that capture both structural and contextual aspects, while GNNs excel at capturing structural information. This combination allows for a more comprehensive understanding of graph data, leveraging the robust textual understanding of LLMs and the structural information captured by GNNs.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LLM help graph-related tasks?\"\n",
    "query_engine = index.as_query_engine()\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(answer.get_formatted_sources())\n",
    "print(\"query was:\", query)\n",
    "print(\"answer was:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import download_loader\n",
    "\n",
    "# ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "# loader = ArxivReader()\n",
    "# documents = loader.load_data(search_query='id:2311.12399')\n",
    "\n",
    "# #or\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='989e8d6e-f312-43b6-a981-03695eedc3d0', embedding=None, metadata={'page_label': '1', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f07e4085285761d5e7970ae398933808400f7e103ea08bba887d9c735e7f1314', text='A Survey of Graph Meets Large Language Model: Progress and Future Directions\\nYuhan Li1∗,Zhixun Li2∗,Peisong Wang3∗,Jia Li1†,Xiangguo Sun2,\\nHong Cheng2,Jeffrey Xu Yu2\\n1The Hong Kong University of Science and Technology (Guangzhou)\\n2The Chinese University of Hong Kong\\n3Tsinghua University\\nAbstract\\nGraph plays a significant role in representing and\\nanalyzing complex relationships in real-world appli-\\ncations such as citation networks, social networks,\\nand biological data. Recently, Large Language Mod-\\nels (LLMs), which have achieved tremendous suc-\\ncess in various domains, have also been leveraged\\nin graph-related tasks to surpass traditional Graph\\nNeural Networks (GNNs) based methods and yield\\nstate-of-the-art performance. In this survey, we first\\npresent a comprehensive review and analysis of ex-\\nisting methods that integrate LLMs with graphs.\\nFirst of all, we propose a new taxonomy, which or-\\nganizes existing methods into three categories based\\non the role (i.e., enhancer, predictor, and alignment\\ncomponent) played by LLMs in graph-related tasks.\\nThen we systematically survey the representative\\nmethods along the three categories of the taxonomy.\\nFinally, we discuss the remaining limitations of ex-\\nisting studies and highlight promising avenues for\\nfuture research. The relevant papers are summarized\\nand will be consistently updated at: https://github.\\ncom/yhLeeee/Awesome-LLMs-in-Graph-tasks.\\n1 Introduction\\nGraph, or graph theory, serves as a fundamental part of nu-\\nmerous areas in the modern world, particularly in technology,\\nscience, and logistics [Ji et al. , 2021]. Graph data represents\\nthe structural characteristics between nodes, thus illuminating\\nrelationships within the graph’s components. Many real-world\\ndatasets, such as citation networks [Sen et al. , 2008], social net-\\nworks [Hamilton et al. , 2017], and molecular [Wu et al. , 2018],\\nare intrinsically represented as graphs. To tackle graph-related\\ntasks, Graph Neural Networks (GNNs) [Kipf and Welling,\\n2016; Velickovic et al. , 2018] have emerged as one of the\\nmost popular choices for processing and analyzing graph data.\\nThe main objective of GNNs is to acquire expressive repre-\\nsentations at the node, edge, or graph level for different kinds\\nof downstream tasks through recursive message passing and\\naggregation mechanisms among nodes.\\n∗Equal Contribution\\n†Corresponding author (jialee@ust.hk).\\nCitation Network\\n···Graph Neural \\nNetwork\\nLarge Language\\nModel\\n?Multi-domain\\nGraph Datasets\\nGraph+LLMs\\nLink\\nPrediction\\nClassification\\nReasoning\\nRecommendation\\n···Downstream Tasks\\nSocial Network\\nMolecular Graph\\nWeb Link\\nFigure 1: Across a myriad of graph domains, the integration of graphs\\nand LLMs demonstrates success in various downstream tasks.\\nIn recent years, significant advancements have been made in\\nLarge Language Models (LLMs) like Transformers [Vaswani\\net al. , 2017], BERT [Kenton and Toutanova, 2019], GPT\\n[Brown et al. , 2020], and their variants. These LLMs can\\nbe easily applied to various downstream tasks with little adap-\\ntation, demonstrating remarkable performance across various\\nnatural language processing tasks, such as sentiment analy-\\nsis, machine translation, and text classification [Zhao et al. ,\\n2023d]. While their primary focus has been on text sequences,\\nthere is a growing interest in enhancing the multi-modal capa-\\nbilities of LLMs to enable them to handle diverse data types,\\nincluding graphs [Chai et al. , 2023], images [Zhang et al. ,\\n2023b], and videos [Zhang et al. , 2023a].\\nLLMs help graph-related tasks. With the help of LLMs,\\nthere has been a notable shift in the way we interact with\\ngraphs, particularly those containing nodes associated with\\ntext attributes. As shown in Figure 1, the integration of graphs\\nand LLMs demonstrates success in various downstream tasks\\nacross a myriad of graph domains. Integrating LLMs with tra-\\nditional GNNs can be mutually beneficial and enhance graph\\nlearning. While GNNs are proficient at capturing structural\\ninformation, they primarily rely on semantically constrained\\nembeddings as node features, limiting their ability to express\\nthe full complexities of the nodes. Incorporating LLMs, GNNs\\ncan be enhanced with stronger node features that effectively\\ncapture both structural and contextual aspects. On the other\\nhand, LLMs excel at encoding text but often struggle to capture\\nstructural information present in graph data. Combining GNNs\\nwith LLMs can leverage the robust textual understanding ofarXiv:2311.12399v2  [cs.LG]  28 Nov 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ccb5a1c-0d93-4bd7-8056-e9fba937865b', embedding=None, metadata={'page_label': '2', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ae4c455a4df007c677b5c1706c159458a0fcb20ed2b0ebd8263efeaad455cae2', text='LLMs while harnessing GNNs’ ability to capture structural\\nrelationships, leading to more comprehensive and powerful\\ngraph learning. For example, TAPE [He et al. , 2023] lever-\\nages semantic knowledge that is relevant to the nodes (i.e.,\\npapers) generated by LLMs to improve the quality of initial\\nnode embeddings in GNNs. In addition, InstructGLM [Ye\\net al. , 2023] replaces the predictor from GNNs with LLMs,\\nleveraging the expressive power of natural language through\\ntechniques such as flattening graphs and designing instruc-\\ntion prompts. MoleculeSTM [Liu et al. , 2022] aligns GNNs\\nand LLMs into the same vector space to introduce textual\\nknowledge into graphs (i.e., molecules), thereby improving\\nreasoning abilities.\\nIt is evident that LLMs have a significant influence on graph-\\nrelated tasks from different perspectives. To achieve a better\\nsystematic overview, as shown in Figure 2, we follow Chen et\\nal.[2023a] to organize our first-level taxonomy, categorizing\\nbased on the role (i.e., enhancer, predictor, and alignment\\ncomponent) played by LLMs throughout the entire model\\npipeline. We further refine our taxonomy and introduce more\\ngranularity to the initial categories.\\nMotivations. Although LLMs have been increasingly ap-\\nplied in graph-related tasks, this rapidly expanding field still\\nlacks a systematic review. Zhang et al. [2023d] conducts a\\nforward-looking survey, presenting a perspective paper that\\ndiscusses the challenges and opportunities associated with the\\nintegration of graphs and LLMs. Liu et al. [2023b] provide an-\\nother related survey that summarizes existing graph foundation\\nmodels and offers an overview of pre-training and adaptation\\nstrategies. However, both of them have limitations in terms\\nof comprehensive coverage and the absence of a taxonomy\\nspecifically focused on how LLMs enhance graphs. In con-\\ntrast, we concentrate on scenarios where both graph and text\\nmodalities coexist and propose a more fine-grained taxonomy\\nto systematically review and summarize the current status of\\nLLMs techniques for graph-related tasks.\\nContributions. The contributions of this work can be sum-\\nmarized from the following three aspects. (1)A structured\\ntaxonomy . A broad overview of the field is presented with a\\nstructured taxonomy that categorizes existing works into four\\ncategories (Figure 2). (2)A comprehensive review . Based\\non the proposed taxonomy, the current research progress of\\nLLMs for graph-related tasks is systematically delineated. (3)\\nSome future directions . We discuss the remaining limitations\\nof existing works and point out possible future directions.\\n2 Preliminary\\nIn this section, we first introduce the basic concepts of two\\nkey areas related to this survey, i.e., GNNs and LLMs. Next,\\nwe give a brief introduction to the newly proposed taxonomy.\\n2.1 Graph Neural Networks\\nDefinitions. Most existing GNNs follow the message-passing\\nparadigm which contains message aggregation and feature\\nupdate, such as GCN [Kipf and Welling, 2016] and GAT\\n[Velickovic et al. , 2018]. They generate node representations\\nby iteratively aggregating information of neighbors and updat-\\ning them with non-linear functions. The forward process canbe defined as:\\nh(l)\\ni=U\\x10\\nh(l−1)\\ni,M({h(l−1)\\ni, h(l−1)\\nj|vj∈ Ni})\\x11\\nwhere h(l)\\niis the feature vector of node iin the l-th layer,\\nandNiis a set of neighbor nodes of node i.Mdenotes the\\nmessage passing function of aggregating neighbor information,\\nUdenotes the update function with central node feature and\\nneighbor node features as input. By stacking multiple layers,\\nGNNs can aggregate messages from higher-order neighbors.\\nGraph pre-training and prompting. While GNNs have\\nachieved some success in graph machine learning, they re-\\nquire expensive annotations and barely generalize to unseen\\ndata. To remedy these deficiencies, graph pre-training aims to\\nextract some general knowledge for the graph models to easily\\ndeal with different tasks without significant annotation cost.\\nThe current mainstream graph pertaining methods can be di-\\nvided into contrastive and generative approaches. For instance,\\nGraphCL [You et al. , 2020] and GCA [Zhu et al. , 2021] follow\\na contrastive learning framework and maximize the agreement\\nbetween two augmented views. Sun et al. [2023b] extend\\nthe contrastive idea to hypergraphs. GraphMAE [Hou et al. ,\\n2022], S2GAE [Tan et al. , 2023a], and WGDN [Cheng et\\nal., 2023] mask the component of the graph and attempt to\\nreconstruct the original data. The typical learning scheme of\\n“pre-training and fine-tuning” is based on the assumption that\\nthe pre-training task and downstream tasks share some com-\\nmon intrinsic task space. Instead, in the NLP area, researchers\\ngradually focus on a new paradigm of “pre-training, prompt-\\ning, and fine-tuning”, which aims to reformulate input data to\\nfit the pretext. This idea has also been naturally applied to the\\ngraph learning area. GPPT [Sun et al. , 2022] first pre-trains\\ngraph model by masked edge prediction, then modify the stan-\\ndalone node into a token pair and reformulate the downstream\\nclassification as edge prediction task. Additionally, All in One\\n[Sun et al. , 2023a] proposes a multi-task prompting frame-\\nwork, which unifies the format of graph prompts and language\\nprompts.\\n2.2 Large Language Models\\nDefinitions. While there is currently no clear definition for\\nLLMs [Shayegani et al. , 2023], here we provide a specific\\ndefinition for LLMs mentioned in this survey. Two influen-\\ntial surveys on LLMs [Zhao et al. , 2023d; Yang et al. , 2023]\\ndistinguish between LLMs and pre-trained language models\\n(PLMs) from the perspectives of model size and training ap-\\nproach. To be specific, LLMs are those huge language models\\n(i.e., billion-level) that undergo pre-training on a significant\\namount of data, whereas PLMs refer to those early pre-trained\\nmodels with moderate parameter sizes (i.e., million-level),\\nwhich can be easily further fine-tuned on task-specific data to\\nachieve better results to downstream tasks. Due to the rela-\\ntively smaller parameter size of GNNs, incorporating GNNs\\nand LLMs often does not require LLMs with large parameters.\\nHence, we follow Liu et al. [2023b] to extend the definition of\\nLLMs in this survey to encompass both LLMs and PLMs as\\ndefined in previous surveys.\\nEvolution. LLMs can be divided into two categories based\\non non-autoregressive and autoregressive language modeling.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ab1ce3c-136b-47fe-8bb0-0887a2f4cb06', embedding=None, metadata={'page_label': '3', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='4083939ae180dab87fa9e7a5c772ba1590d95a03193c26a84b54706baef08372', text='Graph Meets Large Language ModelLLM as EnhancerExplanation-basedTAPE [He et al. , 2023], KEA [Chen et al. , 2023a], LLM4Mol [Qian et al. , 2023],\\nLLMRec [Wei et al. , 2023]\\nEmbedding-basedGIANT [Chien et al. , 2021], SimTeG [Duan et al. , 2023], TouchUp-G [Zhu et al. , 2023],\\nWalkLM [Tan et al. , 2023b], OFA [Liu et al. , 2023a],G ALM [Xie et al. , 2023],\\nG-Prompt [Huang et al. , 2023b]\\nLLM as PredictorFlatten-basedFrozenNLGraph [Wang et al. , 2023b], [Chen et al. , 2023a], GPT4Graph [Guo et al. , 2023],\\n[Liu and Wu, 2023], LLM4DyG [Zhang et al. , 2023c], GraphText [Zhao et al. , 2023c],\\n[Fatemi et al. , 2023], [Hu et al. , 2023], ReLM [Shi et al. , 2023], [Huang et al. , 2023a]\\nTuning GIMLET [Zhao et al. , 2023a], InstructGLM [Ye et al. , 2023]\\nGNN-basedGIT-Mol [Liu et al. , 2023c], GraphLLM [Chai et al. , 2023], MolCA [Liu et al. , 2023d],\\nGraphGPT [Tang et al. , 2023], DGTL [Qin et al. , 2023]\\nGNN-LLM AlignmentSymmetricalSAFER [Chandra et al. , 2020], Text2Mol [Edwards et al. , 2021], MoMu [Su et al. , 2022],\\nMoleculeSTM [Liu et al. , 2022], GLEM [Zhao et al. , 2022], G2P2 [Wen and Fang, 2023],\\nConGraT [Brannon et al. , 2023], G RENADE [Liet al. , 2023b]\\nAsymmetrical GraphFormers [Yang et al. , 2021], G RAD [Mavromatis et al. , 2023], Patton [Jin et al. , 2023]\\nOthersLLM as Annotator LLM-GNN [Chen et al. , 2023b]\\nLLM as Controller GPT4GNAS [Wang et al. , 2023a]\\nLLM as Sample Generator ENG [Yu et al. , 2023]\\nFigure 2: A taxonomy of models for solving graph tasks with the help of large language models (LLMs) with representative examples.\\nNon-autoregressive LLMs typically concentrate on natural\\nlanguage understanding and employ a “masked language mod-\\neling” pre-training task, while autoregressive LLMs focus\\nmore on natural language generation, frequently leveraging\\nthe “next token prediction” objective as their foundational\\ntask. Classic encoder-only models such as BERT [Kenton\\nand Toutanova, 2019], SciBERT [Beltagy et al. , 2019], and\\nRoBERTa [Liu et al. , 2019] fall under the category of non-\\nautoregressive LLMs. Recently, autoregressive LLMs have\\nwitnessed continuous development. Examples include Flan-\\nT5 [Chung et al. , 2022] and ChatGLM [Zeng et al. , 2022],\\nwhich are built upon the encoder-decoder structure, as well as\\nGPT-3 [Brown et al. , 2020], PaLM [Chowdhery et al. , 2022],\\nGalactica [Taylor et al. , 2022], and LLaMA [Touvron et al. ,\\n2023], which are based on decoder-only architectures. Signifi-\\ncantly, advancements in architectures and training methodolo-\\ngies of LLMs have given rise to emergent capabilities [Wei\\net al. , 2022a], which is the ability to handle complex tasks in\\nfew-shot or zero-shot scenarios via some techniques such as\\nin-context learning [Radford et al. , 2021; Dong et al. , 2022]\\nand chain-of-thought [Wei et al. , 2022b].\\n2.3 Proposed Taxonomy\\nWe propose a taxonomy (as illustrated in Figure 2) that or-\\nganizes representative techniques involving both graph and\\ntext modalities into three main categories: (1)LLM as En-\\nhancer, where LLMs are used to enhance the classification\\nperformance of GNNs. (2)LLM as Predictor, where LLMs\\nutilize the input graph structure information to make predic-\\ntions. (3)GNN-LLM Alignment, where LLMs semantically\\nenhance GNNs through alignment techniques. We note that\\nin some models, due to the rarity of LLMs’ involvement, it\\nbecomes difficult to categorize them into these three main\\nclasses. Therefore, we separately organize them into the “Oth-\\ners” category and provide their specific roles in Figure 2. For\\nexample, LLM-GNN [Chen et al. , 2023b] actively selectsnodes for ChatGPT to annotate, thereby augmenting the GNN\\ntraining by utilizing the LLM as an annotator . GPT4GNAS\\n[Wang et al. , 2023a] considers the LLM as an experienced\\ncontroller in the task of graph neural architecture search. It\\nutilizes GPT-4 [OpenAI, 2023] to explore the search space\\nand generate novel GNN architectures. Furthermore, ENG\\n[Yuet al. , 2023] empowers the LLM as a sample generator\\nto generate additional training samples with labels to provide\\nsufficient supervision signals for GNNs.\\nIn the following sections, we present a comprehensive sur-\\nvey along the three main categories of our taxonomy for incor-\\nporating LLMs into graph-related tasks, respectively.\\n3 LLM as Enhancer\\nGNNs have become powerful tools to analyze graph-structure\\ndata. However, the most mainstream benchmark datasets (e.g.,\\nCora [Yang et al. , 2016] and Ogbn-Arxiv [Hu et al. , 2020])\\nadopt naive methods to encode text information in TAGs us-\\ning shallow embeddings, such as bag-of-words, skip-gram\\n[Mikolov et al. , 2013], or TF-IDF [Salton and Buckley, 1988].\\nThis inevitably constrains the performance of GNNs on TAGs.\\nLLM-as-enhancer approaches correspond to enhancing the\\nquality of node embeddings with the help of powerful LLMs.\\nThe derived embeddings are attached to the graph structure\\nto be utilized by any GNNs or directly inputted into down-\\nstream classifiers for various tasks. We naturally categorize\\nthese approaches into two branches: explanation-based and\\nembedding-based, depending on whether they use LLMs to\\nproduce additional textual information.\\n3.1 Explanation-based Enhancement\\nTo enrich the textual attributes, explanation-based enhance-\\nment approaches focus on utilizing the strong zero-shot capa-\\nbility of LLMs to capture higher-level information. As shown\\nin Figure 3(a), generally they prompt LLMs to generate seman-\\ntically enriched additional information, such as explanations,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebe449aa-3bec-49d7-8109-ae13e204068f', embedding=None, metadata={'page_label': '4', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7bdf1d4ed7aa6e92fcc0c8bf2dcb9e1c944a94307092d9abbedef5a753e13de8', text='LLMText Attributes\\nExplanation\\nLM\\nGNN\\nEmbeddingsLLM\\nGNN\\nEmbeddingsText Attributes\\n= Tuned\\n= Frozen= Optional OperationGraphStructure(a) Explanation-based(b) Embedding-basedFigure 3: The illustration of LLM-as-enhancer approaches: a)\\nexplanation-based enhancement , which uses LLMs to generate\\nexplanations of text attributes to enhance text embeddings; b)\\nEmbedding-based enhancement , which directly obtains text em-\\nbeddings by LLMs as initial node embeddings.\\nknowledge entities, and pseudo labels. The typical pipeline is\\nas follows:\\nEnhancement: ei=fLLM(ti, p),xi=fLM(ei, ti),\\nGraph Learning: H=fGNN(X,A),\\nwhere tiis the original text attributes, pis the designed tex-\\ntual prompts, eiis the additional textual output of LLMs,\\nxi∈RDandX∈RN×Ddenotes the enhanced initial node\\nembedding of node iwith the dimension Dand embedding\\nmatrix, along with adjacency matrix A∈RN×Nto obtain\\nnode representations H∈RN×dby GNNs, where dis the\\ndimension of representations. For instance, TAPE [He et al. ,\\n2023] is a pioneer work of explanation-based enhancement,\\nwhich prompts LLMs to generate explanations and pseudo\\nlabels to augment textual attributes. After that, relatively small\\nlanguage models are fine-tuned on both original text data and\\nexplanations to encode text semantic information as initial\\nnode embeddings. Chen et al. [2023a] explore the potential\\ncompetence of LLMs in graph learning. They first compare\\nembedding-visible LLMs with shallow embedding methods\\nand then propose KEA to enrich the text attributes. KEA\\nprompts LLMs to generate a list of knowledge entities along\\nwith text descriptions and encodes them by fine-tuned PLMs\\nand deep sentence embedding models. LLM4Mol [Qian et\\nal., 2023] attempts to employ LLMs to assist in molecular\\nproperty prediction. Specifically, it uses LLMs to generate\\nsemantically enriched explanations for the original SMILES\\nand then fine-tunes a small-scale language model to conduct\\ndownstream tasks. LLMRec [Wei et al. , 2023] aims to utilize\\nLLMs to figure out data sparsity and data quality issues in\\nthe graph recommendation system. It reinforces user-item\\ninteraction edges and generates user/item side information by\\nLLMs. Lastly, it employs a lightweight GNN [He et al. , 2020]\\nto encode the augmented recommendation network.\\n3.2 Embedding-based Enhancement\\nRefer to Figure 3(b), embedding-based enhancement ap-\\nproaches directly utilize LLMs to output text embeddingsas initial node embeddings for GNN training:\\nEnhancement: xi=fLLM(ti),\\nGraph Learning: H=fGNN(X,A).\\nThis kind of approach requires the use of embedding-visible or\\nopen-source LLMs because it needs to access text embeddings\\nstraightaway and fine-tune LLMs with structural information.\\nMany of the current advanced LLMs (e.g., GPT4 [OpenAI,\\n2023] and PaLM [Chowdhery et al. , 2022]) are closed-source\\nand only provide online services. Strict restrictions prevent\\nresearchers from accessing their parameters and output embed-\\ndings. This kind of approach mostly adopts a cascading form\\nand utilizes structure information to assist the language model\\nin pre-training or fine-tuning. Typically, G ALM [Xie et al. ,\\n2023] pre-trains PLMs and GNN aggregator on a given large\\ngraph corpus to capture the information that can maximize\\nutility towards massive applications and then fine-tunes the\\nframework on a specific downstream application to further\\nimprove the performance.\\nSeveral works aim to generate node embeddings by incor-\\nporating structural information into the fine-tuning phase of\\nLLMs. Representatively, GIANT [Chien et al. , 2021] fine-\\ntunes the language model by a novel self-supervised learning\\nframework, which employs XR-Transformers to solve extreme\\nmulti-label classification over link prediction. SimTeG [Duan\\net al. , 2023] and TouchUp-G [Zhu et al. , 2023] follow a simi-\\nlar way, they both fine-tune PLMs through link-prediction-like\\nmethods to help them perceive structural information. The\\nsubtle difference between them is that TouchUp-G uses nega-\\ntive sampling during link prediction, while SimTeG employs\\nparameter-efficient fine-tuning to accelerate the fine-tuning\\nprocess. G-Prompt [Huang et al. , 2023b] introduces a graph\\nadapter at the end of PLMs to help extract graph-aware node\\nfeatures. Once trained, task-specific prompts are incorporated\\nto produce interpretable node representations for various down-\\nstream tasks. WalkLM [Tan et al. , 2023b] is an unsupervised\\ngeneric graph representation learning method. The first step\\nof it is to generate attributed random walks on the graph and\\ncompose roughly meaningful textual sequences by automated\\ntextualization program. The second step is to fine-tune an\\nLLM using textual sequences and extract representations from\\nLLM.\\nA recent work, OFA [Liu et al. , 2023a], attempts to pro-\\npose a general graph learning framework, which can utilize a\\nsingle graph model to conduct adaptive downstream predic-\\ntion. It describes all nodes and edges using human-readable\\ntexts and encodes them from different domains into the same\\nspace by LLMs. Subsequently, the framework is adaptive to\\nperform different tasks by inserting task-specific prompting\\nsubstructures into the input graph.\\n3.3 Discussions\\nLLM-as-enhancer approaches have demonstrated superior per-\\nformance on TAG, being able to effectively capture both tex-\\ntual and structural information. Moreover, they also exhibit\\nstrong flexibility, as GNNs and LLMs are plug-and-play, al-\\nlowing them to leverage the latest techniques to address the en-\\ncountered issues. Another advantage of such methods (specif-\\nically explanation-based enhancement) is that they pave the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e1362a9-3977-4672-a2ca-01299c9c1d81', embedding=None, metadata={'page_label': '5', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='84f68613b1931b19b62ed052a89da7f5e86a7a0a1881162b2c9934af36c05c18', text='(a) Flatten-based(b) GNN-basedText Attributes\\nGraphStructureGNN\\nInitialFeaturesGraphEmbeddingsLLM\\nGraphStructureFlatteningLLM\\n1-hop:\\n2-hop:\\nSequenceFigure 4: The illustration of LLM-as-predictor approaches: a)\\nFlatten-based prediction , which incorporates graph structure with\\nLLMs via different flattening strategies; b) GNN-based prediction ,\\nutilizing GNNs to capture structural information for LLMs.\\nway for using closed-source LLMs to assist graph-related tasks.\\nHowever, despite some papers claiming strong scalability, in\\nfact, LLM-as-enhancer approaches entail significant overhead\\nwhen dealing with large-scale datasets. Taking explanation-\\nbased approaches as an example, they need to query LLMs’\\nAPIs for Ntimes for a graph with Nnodes, which is indeed a\\nsubstantial cost.\\n4 LLM as Predictor\\nThe core idea behind this category is to utilize LLMs to make\\npredictions for a wide range of graph-related tasks, such as\\nclassifications and reasonings, within a unified generative\\nparadigm. However, applying LLMs to graph modalities\\npresents unique challenges, primarily because graph data of-\\nten lacks straightforward transformation into sequential text,\\nas different graphs define structures and features in differ-\\nent ways. In this section, we classify the models broadly\\ninto flatten-based and GNN-based predictions, depending on\\nwhether they employ GNNs to extract structural features for\\nLLMs.\\n4.1 Flatten-based Prediction\\nThe majority of the existing attempts that utilize LLMs as pre-\\ndictors employ the strategy of flattening the graph into textual\\ndescriptions, which facilitates direct processing of graph data\\nby LLMs through text sequences. As shown in Figure 4(a),\\nflatten-based prediction typically involves two steps: (1)utiliz-\\ning a flatten function Flat (·)to transform a graph structure\\ninto a sequence of nodes or tokens Gseq, and (2)a parsing\\nfunction Parse (·)is then applied to retrieve the predicted\\nlabel from the output generated by LLMs, as illustrated below:\\nGraph Flattening: Gseq=Flat (V,E,T,J),\\nPrediction: ˜Y=Parse (fLLM(Gseq, p)),\\nwhere V,E,T, andJdenotes the set of nodes, edges, node\\ntext attributes, and edge text attributes, respectively. pindi-\\ncates the instruction prompt for the current graph task and ˜Y\\nis the predicted label.\\nThe parsing strategies of models are generally standardized.\\nFor example, given that the output of LLMs often involvestheir reasoning and logic processes, particularly in the chain-\\nof-thought (CoT) scenario, several works [Fatemi et al. , 2023;\\nZhao et al. , 2023c; Chen et al. , 2023a; Guo et al. , 2023; Liu\\nand Wu, 2023; Wang et al. , 2023b] utilize regular expressions\\nto extract the predicted label from the output. Some models\\n[Chen et al. , 2023a; Fatemi et al. , 2023; Wang et al. , 2023b;\\nChai et al. , 2023; Huang et al. , 2023a] further set the decoding\\ntemperature of the LLM to 0, in order to reduce the variance\\nof LLM’s predictions and obtain more reliable results. An-\\nother direction is to formulate graph tasks as multi-choice QA\\nproblems [Robinson and Wingate, 2022] where LLMs are in-\\nstructed to select the correct answer among provided choices.\\nFor instance, some works [Huang et al. , 2023a; Hu et al. , 2023;\\nShiet al. , 2023] constrain LLM’s output format via giving\\nchoices and appending instructions in prompts in zero-shot\\nsetting, such as “ Do not give any reasoning or logic for your\\nanswer ”. In addition, some methods, such as GIMLET [Zhao\\net al. , 2023a] and InstructGLM [Ye et al. , 2023], fine-tune\\nLLMs to directly output predicted labels, empowering them\\nto provide accurate predictions without the need for additional\\nparsing steps.\\nCompared to parsing strategies, flattening strategies can\\nexhibit significant variation. In the following, we organize\\nthe methods for flattening based on whether the parameters of\\nLLMs are updated.\\nLLM Frozen\\nGPT4Graph [Guo et al. , 2023] utilizes graph description lan-\\nguages such as GML [Himsolt, 1997] and GraphML [Brandes\\net al. , 2013] to represent graphs. These languages provide\\nstandardized syntax and semantics for representing the nodes\\nand edges within a graph. Inspired by linguistic syntax trees\\n[Chiswell and Hodges, 2007], GraphText [Zhao et al. , 2023c]\\nleverages graph-syntax trees to convert a graph structure to\\na sequence of nodes, which is then fed to LLMs for training-\\nfree graph reasoning. Furthermore, ReLM [Shi et al. , 2023]\\nuses simplified molecular input line entry system (SMILES)\\nstrings to provide one-dimensional linearizations of molecular\\ngraph structures. Graph data can be also represented through\\nmethods like adjacency matrices and adjacency lists. Several\\nmethods [Wang et al. , 2023b; Fatemi et al. , 2023; Liu and\\nWu, 2023; Zhang et al. , 2023c] directly employ numerically\\norganized node and edge lists to depict the graph data in plain\\ntext.\\nInstead, the use of natural narration to express graph struc-\\ntures is also making steady progress. Chen et al. [2023a] and\\nHuet al. [2023] both integrate the structural information of\\ncitation networks into the prompts, which is achieved by ex-\\nplicitly representing the edge relationship through the word\\n“cite” and representing the nodes using paper indexes or titles.\\nHuang et al. [2023a], on the other hand, does not use the word\\n“cite” to represent edges but instead describes the relationships\\nvia enumerating randomly selected k-hop neighbors of the\\ncurrent node. In addition, GPT4Graph [Guo et al. , 2023] and\\nChen et al. [2023a] imitate the aggregation behavior of GNNs\\nand summarize the current neighbor’s attributes as additional\\ninputs, aiming to provide more structural information. It is\\nworth noting that Fatemi et al. [2023] investigates various\\nmethodologies to represent nodes and edges, examining a total', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8021d137-f9aa-47df-bbfd-c90226d57b29', embedding=None, metadata={'page_label': '6', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e8cbc7dcb31c1cf6d1fe85024cf89dc1b8f436888938c764725fb09d20583d97', text='of 11 strategies. For example, they use indexes or alphabet\\nletters to denote nodes and apply arrows or parentheses to\\nsignify edges.\\nLLM Tuning\\nGIMLET [Zhao et al. , 2023a] adopts distance-based position\\nembedding to extend the capability of LLMs to perceive graph\\nstructures. When performing positional encoding of the graph,\\nGIMLET defines the relative position of two nodes as the\\nshortest distance between them in the graph, which has been\\nwidely utilized in the literature of graph transformers [Ying\\net al. , 2021]. Similar to Huang et al. [2023a], InstructGLM\\n[Yeet al. , 2023] designs a series of scalable prompts based on\\nthe maximum hop level. These prompts allow a central paper\\nnode to establish direct associations with its neighbors up to\\nany desired hop level by utilizing the described connectivity\\nrelationships expressed in natural language.\\n4.2 GNN-based Prediction\\nGNNs have demonstrated impressive capabilities in under-\\nstanding graph structures through recursive information ex-\\nchange and aggregation among nodes. As illustrated in Fig-\\nure 4(b), in contrast to flatten-based prediction, which con-\\nverts graph data into textual descriptions as inputs to LLMs,\\nGNN-based prediction leverages the advantages of GNNs to\\nincorporate inherent structural characteristics and dependen-\\ncies present in graph data with LLMs, allowing LLMs to be\\nstructure-aware as follows:\\nGraph Learning: H=fGNN(X,A),\\nPrediction: ˜Y=Parse (fLLM(H, p)),\\nwhere Xdenotes the node embedding matrix, Ais the adja-\\ncency matrix, and Hdenotes the structure-aware embeddings\\nassociated with the graph. GNN-based prediction also relies\\non a parser to extract the output from LLMs. However, inte-\\ngrating GNN representations into LLMs often requires tuning,\\nmaking it easier to standardize the prediction format of LLMs\\nby providing desirable outputs during training.\\nVarious strategies have been proposed to fuse the structural\\npatterns learned by GNNs and the contextual information cap-\\ntured by LLMs. For instance, GIT-Mol [Liu et al. , 2023c]\\nand MolCA [Liu et al. , 2023d] both implement BLIP-2’s Q-\\nFormer [Li et al. , 2023a] as the cross-modal projector to map\\nthe graph encoder’s output to the LLM’s input text space. Mul-\\ntiple objectives with different attention masking strategies are\\nemployed for effective graph-text interactions. GraphLLM\\n[Chai et al. , 2023] derives the graph-enhanced prefix by ap-\\nplying a linear projection to the graph representation during\\nprefix tuning, allowing the LLM to synergize with the graph\\ntransformer to incorporate structural information crucial to\\ngraph reasoning. Additionally, GraphGPT [Tang et al. , 2023]\\nalso employs a simple linear layer as the lightweight align-\\nment projector to map the encoded graph representation to\\nsome graph tokens, while the LLM excels at aligning these\\ntokens with diverse text information. DGTL [Qin et al. , 2023]\\ninjects the disentangled graph embeddings directly into each\\nlayer of the LLM, highlighting different aspects of the graph’s\\ntopology and semantics.4.3 Discussions\\nUtilizing LLMs directly as predictors shows superiority in\\nprocessing textual attributes of graphs, especially achieving\\nremarkable zero-shot performance compared with traditional\\nGNNs. The ultimate goal is to develop and refine methods\\nfor encoding graph-structured information into a format that\\nLLMs can comprehend and manipulate effectively and effi-\\nciently. Flatten-based prediction may have an advantage in\\nterms of effectiveness, while GNN-based prediction tends to\\nbe more efficient. In flatten-based prediction, the input length\\nlimitation of LLMs restricts each node’s access to only its\\nneighbors within a few hops, making it challenging to capture\\nlong-range dependencies. Additionally, without the involve-\\nment of GNNs, inherent issues of GNNs such as heterophily\\ncannot be addressed. On the other hand, for GNN-based pre-\\ndiction, training an additional GNN module and inserting it\\ninto LLMs for joint training is challenging due to the problem\\nof vanishing gradients in the early layers of deep transformers\\n[Zhao et al. , 2023a; Qin et al. , 2023].\\n5 GNN-LLM Alignment\\nAligning the embedding spaces of GNNs and LLMs is an\\neffective way to integrate the graph modality with the text\\nmodality. GNN-LLM alignment ensures that each encoder’s\\nunique functionalities are preserved while coordinating their\\nembedding spaces at a specific stage. In this section, we sum-\\nmarize the techniques for aligning GNNs and LLMs, which\\ncan be classified as symmetric or asymmetric, depending on\\nwhether equal emphasis is placed on both GNNs and LLMs or\\nif one modality is prioritized over the other.\\n5.1 Symmetric\\nSymmetric alignment refers to the equal treatment of the graph\\nand text modalities during the alignment process. These ap-\\nproaches ensure that the encoders of both modalities achieve\\ncomparable performance in their respective applications.\\nA typical symmetric alignment architecture, illustrated in\\nFigure 5(a), adopts a two-tower style, employing separate en-\\ncoders to individually encode the graph and text. Notice that\\nduring the alignment process, both modalities interact only\\nonce. Some methods, like SAFER [Chandra et al. , 2020],\\nutilize simple concatenation on these separate embeddings.\\nHowever, this approach falls short in achieving a seamless fu-\\nsion of structural and textual information, resulting in a loosely\\ncoupled integration of the two modalities. Consequently, the\\nmajority of two-tower style models utilize contrastive learning\\ntechniques to facilitate alignment, akin to CLIP [Radford et al. ,\\n2021] for aligning visual and language modalities. In general,\\nthe methods encompass two steps. The first step is feature ex-\\ntraction, where graph representations and text representations\\nare obtained. The second step involves the use of a contrastive\\nlearning process with a modified InfoNCE loss [Oord et al. ,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a8579dd-a1c2-49bd-b9d0-fc9e655524b3', embedding=None, metadata={'page_label': '7', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ada238034d2a0eb2a3bf67ed9628ec0dc44ee115a106e837ac7b105b2c421e72', text='Graph\\nStructureInitial\\nFeatures\\nGNN\\nGraph\\nEmbeddings\\nLLMText Attributes\\nText\\nEmbeddings\\nContrastive/\\nConcatenateText Attributes\\nTRM\\nTRM\\nGNN\\nGraph Structure\\nLLMText Attributes\\nText\\nEmbeddings\\nGNN\\nLLM\\nStudent ModelTeacher Model\\n(a) ContrastiveGraph\\nStructureInitial\\nFeatures\\nGNN\\nPseudo-Labels\\nLLMText Attributes\\nE-Step\\nPseudo-LabelsSuperviseM-Step\\n(b) Iterative (c) Graph-Nested (d) DistillationGraph Structure\\nTRM\\nGNN\\nFigure 5: The illustration of GNN-LLM-Alignment approaches: a) Contrastive , symmetric alignment which applies concatenation or\\ncontrastive learning to graph embeddings and text embeddings; b) Iterative , belongs to symmetric alignment, aiming to implement iterative\\ninteractions on embeddings of two modalities; c) Graph-nested , a symmetric alignment which interweaves GNNs with Transformers and d)\\nDistillation , belongs to asymmetric alignment, which uses GNN as a teacher to train language models to be graph-aware.\\n2018] depicted with the following equation:\\nℓ(gi,ti) =−loges(gi,ti)/τ\\nP|G|\\nk=1es(gi,tk)/τ,\\nLInfoNCE =1\\n2|G||G|X\\ni=1\\x10\\nℓ(gi,ti) +ℓ(ti,gi)\\x11\\n,\\nwhere grepresents the representation of a specific graph, while\\ntdenotes the representation of the corresponding text of the\\ngraph. s(·,·)denotes the score function that assigns high\\nvalues to the positive pair, and low values to negative pairs.\\nτis a temperature parameter and |G|denotes the number of\\ngraphs in the training dataset. Parameters of both encoders are\\nupdated via backpropagation based on the contrastive loss.\\nText2Mol [Edwards et al. , 2021] proposes a cross-modal\\nattention mechanism to achieve early fusion of graph and tex-\\ntual embeddings. Implemented through a transformer decoder,\\nText2Mol uses the LLM’s output as a source sequence and\\nthe GNN’s output as a target sequence. This setup allows the\\nattention mechanism to learn multimodal association rules.\\nThe decoder’s output is then utilized for contrastive learning,\\npaired with the processed outputs from the GNN.\\nMoMu [Su et al. , 2022], MoleculeSTM [Liu et al. , 2022],\\nand ConGraT [Brannon et al. , 2023] share a similar framework,\\nwhich adopts paired graph embeddings and text embeddings to\\nimplement contrastive learning, but there are still differences\\nin detail. Both MoMu and MoleculeSTM gather molecules\\nfrom PubChem [Wang et al. , 2009]. However, their methods\\nof obtaining related texts differ. The former retrieves related\\ntexts from published scientific papers, while the latter utilizes\\nthe corresponding descriptions of the molecules. ConGraT\\nexpands this architecture beyond the molecular domain. It has\\nvalidated this graph-text paired contrastive learning method\\non social, knowledge, and citation networks.\\nSeveral studies such as G2P2 [Wen and Fang, 2023] and\\nGRENADE [Liet al. , 2023b] have further advanced the use of\\ncontrastive learning. Specifically, G2P2 enhances the granular-\\nity of contrastive learning and introduces prompts during the\\nfine-tuning stage. It employs contrastive learning at three lev-\\nels during the pre-training stage: node-text, text-text summary,\\nand node-node summary, thereby strengthening the alignment\\nbetween text and graph representations. Prompts are utilized\\nin downstream tasks, demonstrating strong performance infew-shot and zero-shot text classification and node classifi-\\ncation tasks. On the other hand, G RENADE is optimized by\\nintegrating graph-centric contrastive learning with dual-level\\ngraph-centric knowledge alignment, which includes both node-\\nlevel and neighborhood-level alignment.\\nContrary to previous methods, the iterative alignment ap-\\nproach, depicted in Figure 5(b), treats both modalities equally\\nbut distinguishes itself in the training process by allowing\\nfor iterative interaction between the modalities. For exam-\\nple, GLEM [Zhao et al. , 2022] employs the Expectation-\\nMaximization (EM) framework, where one encoder iteratively\\ngenerates pseudo-labels for the other encoder, allowing them\\nto align their representation spaces.\\n5.2 Asymmetric\\nWhile symmetric alignment aims to give equal emphasis to\\nboth modalities, asymmetric alignment focuses on allowing\\none modality to assist or enhance the other. In current studies,\\nthe predominant approach involves leveraging the capabilities\\nof GNNs to process structural information to reinforce LLMs.\\nThese studies can be categorized into two types: graph-nested\\ntransformer and graph-aware distillation.\\nThe graph-nested transformer, as exemplified by Graph-\\nformer [Yang et al. , 2021] in Figure 5(c), demonstrates asym-\\nmetric alignment through the integration of GNNs into each\\ntransformer layer. Within each layer of the LLM, the node\\nembedding is obtained from the first token-level embedding,\\nwhich corresponds to the [ CLS] token. The process involves\\ngathering embeddings from all relevant nodes and applying\\nthem to a graph transformer. The output is then concatenated\\nwith the input embeddings and passed on to the next layer of\\nthe LLM.\\nPatton [Jin et al. , 2023] extends GraphFormer by propos-\\ning two pre-training strategies, i.e., network-contextualized\\nmasked language modeling and masked node prediction,\\nspecifically for text-rich graphs. Its strong performance is\\nshown in various downstream tasks, including classification,\\nretrieval, reranking, and link prediction.\\nAdditionally, G RAD [Mavromatis et al. , 2023] employs\\ngraph-aware distillation for aligning two modalities, depicted\\nin Figure 5(d). It utilizes a GNN as a teacher model to generate\\nsoft labels for an LLM, facilitating the transfer of aggregated\\ninformation. Moreover, since the LLMs share parameters,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43ea3a64-f158-4d5b-b371-109a9f5919bc', embedding=None, metadata={'page_label': '8', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='95541815490662f2874a8193c37748261e0851aa55f2da034de05bd1aa7f6a93', text='Model GNN LLM Predictor Fine-tuning Prompting Domain Task CodeLLM as EnhancerGIANT [Chien et al. , 2021] SAGE, RevGAT, etc. BERT GNN ✗ ✗ Citation, Co-purchase Node Link\\nGALM [Xie et al. , 2023] RGCN, RGAT BERT GNN ✓ ✗ E-Commerce, Recommendation Node, Link -\\nTAPE [He et al. , 2023] RevGAT ChatGPT GNN ✗ ✓ Citation Node Link\\nChen et al. [Chen et al. , 2023a] GCN, GAT ChatGPT GNN ✗ ✓ Citation, Co-purchase Node -\\nLLM4Mol [Qian et al. , 2023] - ChatGPT LM ✗ ✗ Molecular Graph Link\\nSimTeG [Duan et al. , 2023] SAGE, RevGAT, SEAL allMiniLM-L6-v2, etc. GNN ✓♡✗ Citation, Co-purchase Node, Link Link\\nG-Prompt [Huang et al. , 2023b] SAGE, RevGAT RoBERTa-Large GNN ✓ ✓ Citation, Social Node -\\nTouchUp-G [Zhu et al. , 2023] SAGE, MB-GCN, etc. BERT GNN ✓ ✗ Citation, Co-purchase, Recommendation Node, Link -\\nOFA [Liu et al. , 2023a] R-GCN Sentence-BERT GNN ✗ ✓ Citation, Web link, Knowledge, Molecular Node, Link, Graph Link\\nLLMRec [Wei et al. , 2023] LightGCN ChatGPT GNN ✗ ✓ Recommendation Recommendation Link\\nWalkLM [Tan et al. , 2023b] - DistilRoBERTa MLP ✓ ✗ Knowledge Node, Link LinkLLM as PredictorNLGraph [Wang et al. , 2023b] - Text-davinci-003 LLM ✗ ✓ - Reasoning Link\\nGPT4Graph [Guo et al. , 2023] - Text-davinci-003 LLM ✗ ✓ - Reasoning, Node, Graph Link\\nGIMLET [Zhao et al. , 2023a] - T5 LLM ✓/✗ ✓ Molecular Graph Link\\nChen et al. [Chen et al. , 2023a] - ChatGPT LLM ✗ ✓ Citation Node Link\\nGIT-Mol [Liu et al. , 2023c] GIN MolT5 LLM ✓♡✓ Molecular Graph, Captioning -\\nInstructGLM [Ye et al. , 2023] - FLAN-T5/LLaMA-v1 LLM ✓♡✓ Citation Node Link\\nLiu et al. [Liu and Wu, 2023] - GPT-4, etc. LLM ✗ ✓ - Reasoning Link\\nHuang et al. [Huang et al. , 2023a] - ChatGPT LLM ✗ ✓ Citation, Co-purchase Node Link\\nGraphText [Zhao et al. , 2023c] - ChatGPT/GPT-4 LLM ✗ ✓ Citation, Web link Node -\\nFatemi et al. [Fatemi et al. , 2023] - PaLM/PaLM 2 LLM ✗ ✓ - Reasoning -\\nGraphLLM [Chai et al. , 2023] Graph Transformer LLaMA-v2 LLM ✓♡✓ - Reasoning Link\\nHu et al. [Hu et al. , 2023] - ChatGPT/GPT-4 LLM ✗ ✓ Citation, Knowledge, Social Node, Link, Graph -\\nMolCA [Liu et al. , 2023d] GINE Galactica/MolT5 LLM ✓♡✓ Molecular Graph, Retrieval, Captioning Link\\nGraphGPT [Tang et al. , 2023] Graph Transformer Vicuna LLM ✓♡✓ Citation Node Link\\nReLM [Shi et al. , 2023] TAG, GCN Vicuna/ChatGPT LLM ✗ ✓ Molecular Reaction Prediction Link\\nLLM4DyG [Zhang et al. , 2023c] - Vicuna/LLaMA-v2/ChatGPT LLM ✗ ✓ - Reasoning -\\nDGTL [Qin et al. , 2023] Disentangled GNN LLaMA-v2 LLM ✓ ✓ Citation, E-Commerce Node -GNN-LLM AlignmentSAFER[Chandra et al. , 2020] GCN, GAT, etc. RoBERTa Linear ✓ ✗ News Node Link\\nGraphFormers [Yang et al. , 2021] Graph Transformer UniLM LLM ✓ ✗ Citation, E-Commerce, Knowledge Link Link\\nText2Mol[Edwards et al. , 2021] GCN SciBERT GNN/LLM ✓ ✗ Molecular Retrieval Link\\nMoMu [Su et al. , 2022] GIN BERT GNN/LLM ✓ ✗ Molecular Graph, Retrieval Link\\nMoleculeSTM [Liu et al. , 2022] GIN BERT GNN/LLM ✓ ✗ Molecular Graph, Retrieval Link\\nGLEM [Zhao et al. , 2022] SAGE, RevGAT, etc. DeBERTa GNN/LLM ✓ ✗ Citation, Co-purchase Node Link\\nGRAD [Mavromatis et al. , 2023] SAGE SciBERT/DistilBERT LLM ✓ ✗ Citation, Co-purchase Node Link\\nG2P2 [Wen and Fang, 2023] GCN Transformer GNN/LLM ✓ ✓ Citation, Recommendation Node Link\\nPatton [Jin et al. , 2023] Graph Transformer BERT/SciBERT Linear/LLM ✓ ✗ Citation, E-Commerce Node, Link, Retrieval, Reranking Link\\nConGraT [Brannon et al. , 2023] GAT all-mpnet-base-v2/DistilGPT2 GNN/LLM ✓ ✗ Citation, Knowledge, Social Node, Link Link\\nGRENADE [Liet al. , 2023b] SAGE, RevGAT-KD, etc. BERT GNN/MLP ✓ ✗ Citation, Co-purchase Node, Link LinkOthersLLM-GNN [Chen et al. , 2023b] GCN, SAGE GPT3.5 GNN ✗ ✓ Citation, Co-purchase Node Link\\nGPT4GNAS [Wang et al. , 2023a] GCN, GIN, etc. GPT-4 GNN ✗ ✓ Citation Node -\\nENG [Yu et al. , 2023] GCN, GAT ChatGPT GNN ✗ ✓ Citation Node -\\nTable 1: A summary of models that leverage LLMs to assist graph-related tasks in literature, ordered by their release time. Fine-tuning denotes\\nwhether it is necessary to fine-tune the parameters of LLMs, and ♡indicates that models employ parameter-efficient fine-tuning (PEFT)\\nstrategies, such as LoRA and prefix tuning. Prompting indicates the use of text-formatted prompts in LLMs, done manually or automatically.\\nAcronyms in Task : Node refers to node-level tasks; Link refers to link-level tasks; Graph refers to graph-level tasks; Reasoning refers to Graph\\nReasoning; Retrieval refers to Graph-Text Retrieval; Captioning refers to Graph Captioning.\\nthe GNN can benefit from improved textual encodings after\\nthe updates to the LLMs’ parameters. Through iterative up-\\ndates, a graph-aware LLM is developed, resulting in enhanced\\nscalability in inference due to the absence of the GNN.\\n5.3 Discussions\\nTo align GNNs and LLMs, symmetric alignments treat each\\nmodality equally, with the objective of enhancing GNNs and\\nLLMs simultaneously. This leads to encoders that can effec-\\ntively handle tasks involving both modalities, leveraging their\\nindividual encoding strengths to improve modality-specific\\nrepresentations. In addition, asymmetric methods enhance\\nLLMs by inserting graph encoders into transformers or directly\\nusing GNNs as teachers. However, alignment techniques face\\nchallenges when dealing with data scarcity. In particular, only\\na few graph datasets (i.e., molecular datasets) contain native\\ngraph-text pairs, limiting the applicability of these methods.\\n6 Future Directions\\nTable 1 summarizes the models that leverage LLMs to as-\\nsist graph-related tasks according to the proposed taxonomy.\\nBased on the above review and analysis, we believe that there\\nis still much space for further enhancement in this field. In\\nthis section, we discuss the remaining limitations of leverag-\\ning LLM’s ability to comprehend graph data and list some\\ndirections for further exploration in subsequent research.\\nDealing with non-TAG. Utilizing LLMs to assist learningon text-attributed graphs has already demonstrated excellent\\nperformance. However, graph-structured data is ubiquitous in\\nreal-world scenarios, and a great deal of it lacks rich tex-\\ntual information. For example, in a traffic network (e.g.,\\nPeMS03 [Song et al. , 2020]), each node represents an opera-\\ntional sensor, while in a superpixel graph (e.g., PascalVOC-SP\\n[Dwivedi et al. , 2022]), each node represents a superpixel.\\nThese datasets do not have attached text attributes on each\\nnode, and it is also challenging to describe the semantic mean-\\ning of each node using human-understandable language. Al-\\nthough OFA [Liu et al. , 2023a] proposes to describe all nodes\\nand edges using human-understandable texts and embed the\\ntexts into the same space by LLMs, it may not be applicable to\\nall domains (e.g., superpixel graph), and its performance may\\nbe suboptimal in certain domains and datasets. Exploring how\\nto leverage the powerful generalization capabilities of LLMs\\nto help in constructing graph foundation models is a valuable\\nresearch direction.\\nDealing with data leakage. Data leakage in LLMs has be-\\ncome a focal point of discussion [Aiyappa et al. , 2023]. Given\\nthat LLMs undergo pre-training on extensive text corpora, it’s\\nlikely that LLMs may have seen and memorized at least part of\\nthe test data of the common benchmark datasets, especially for\\ncitation networks. This undermines the reliability of current\\nstudies that rely on earlier benchmark datasets. In addition,\\nChen et al. [2023a] proves that specific prompts could po-\\ntentially enhance the “activation” of LLMs’ corresponding', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8bdec4b1-f04d-4f73-960c-48359c1e6db5', embedding=None, metadata={'page_label': '9', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='123843ab29ea1e5fcd555dcfa8ad8dc5dc74fc5e4ac18955afa315f91e864009', text='memory, thereby influencing the evaluation. Both Huang et al.\\n[2023a] and He et al. [2023] have tried to avoid the data leak-\\nage issue by collecting a new citation dataset, ensuring that the\\ntest papers are sampled from time periods post the data cut-off\\nof ChatGPT. However, they still remain limited to the citation\\ndomain and the impact of graph structures in their datasets is\\nnot significant. Hence, it’s crucial to reconsider the methods\\nemployed to accurately evaluate the performance of LLMs on\\ngraph-related tasks. A fair, systematic, and comprehensive\\nbenchmark is also needed.\\nImproving transferability. Transferability has always been a\\nchallenging problem in the graph domain [Jiang et al. , 2022].\\nThe transferability of learned knowledge from one dataset to\\nanother, or from one domain to another, is not straightforward\\ndue to the unique characteristics and structures of individual\\ngraphs. Graphs can vary significantly in terms of size, connec-\\ntivity, node types, edge types, and overall topology, making it\\ndifficult to directly transfer knowledge between them. While\\nLLMs have demonstrated promising zero/few-shot abilities\\nin language tasks due to their extensive pre-training on vast\\namounts of corpora, the exploration of utilizing the knowl-\\nedge embedded within LLMs to enhance the transferability of\\ngraph-related tasks has been relatively limited. OFA [Liu et\\nal., 2023a] attempts a unified way to perform cross-domain on\\ngraphs by describing all nodes and edges as human-readable\\ntexts and embedding the texts from different domains into\\nthe same embedding space with a single LLM. The topic of\\nimproving transferability is still worth investigating.\\nImproving explainability. Explainability, also known as in-\\nterpretability, denotes the ability to explain or present the\\nbehavior of models in human-understandable terms [Zhao\\net al. , 2023b]. LLMs exhibit improved explainability com-\\npared to GNNs when handling graph-related tasks, primarily\\ndue to the reasoning and explaining ability of LLMs to pro-\\nduce user-friendly explanations for graph reasoning, including\\ngenerating additional explanations as enhancers discussed in\\nSection 3 and offering reasoning processes as predictors dis-\\ncussed in Section 4. Several studies have examined explaining\\ntechniques within the prompting paradigm, such as in-context\\nlearning [Radford et al. , 2021] and chain-of-thought [Wei et\\nal., 2022b], which involve feeding a sequence of demonstra-\\ntions and prompts to the LLM to steer its generation in a\\nparticular direction and have it explain its reasoning. Further\\nexplorations should be conducted to enhance explainability.\\nImproving efficiency. While LLMs have demonstrated their\\neffectiveness in learning on graphs, they may face inefficien-\\ncies in terms of time and space, particularly compared to\\ndedicated graph learning models such as GNNs that inherently\\nprocess graph structures. This is especially obvious when\\nLLMs rely on sequential graph descriptions for predictions\\ndiscussed in Section 4. For example, while accessing LLMs\\nthrough APIs (i.e., ChatGPT and GPT-4), the billing model in-\\ncurs high costs for processing large-scale graphs. Additionally,\\nboth training and inference for locally deployed open-source\\nLLMs require significant time consumption and substantial\\nhardware resources. Existing studies [Duan et al. , 2023; Liu\\net al. , 2023c; Ye et al. , 2023; Chai et al. , 2023; Liu et al. ,\\n2023d; Tang et al. , 2023] have tried to enable LLMs’ efficientadaption via adopting parameter-efficient fine-tuning strate-\\ngies, such as LoRA [Hu et al. , 2021] and prefix tuning [Li\\nand Liang, 2021]. We believe that more efficient methods may\\nunlock more power of applying LLMs on graph-related tasks\\nwith limited computational resources.\\nAnalysis and improvement of expressive ability. Despite\\nthe recent achievements of LLMs in graph-related tasks, their\\ntheoretical expressive power remains largely unexplored. It\\nis widely acknowledged that standard message-passing neu-\\nral networks are as expressive as the 1-Weisfeiler-Lehman\\n(WL) test, meaning that they fail to distinguish non-isomorphic\\ngraphs under 1-hop aggregation [Xu et al. , 2018]. Therefore,\\ntwo fundamental questions arise: How effectively do LLMs\\nunderstand graph structures? Can their expressive ability sur-\\npass those of GNNs or the WL-test? Besides, permutation\\nequivariance is an intriguing property of typical GNNs, which\\nis significant in geometric graph learning [Han et al. , 2022].\\nExploring how to endow LLMs with this property is also an\\ninteresting direction.\\nLLMs as agent. In the current integration of graphs with\\nLLMs, LLMs often play the role of enhancers, predictors, and\\nalignment components. However, in more complex scenarios,\\nsuch applications may not fully unlock the potential of LLMs.\\nRecent research has explored new roles for LLMs as agents,\\nsuch as generative agents [Park et al. , 2023] and domain-\\nspecific agents [Bran et al. , 2023]. In an LLM-powered agent\\nsystem, LLMs function as the agent’s brain, supported by\\nessential components like planning, memory, and tool using\\n[Weng, 2023]. In complex graph-related scenarios, such as\\nrecommendation systems and knowledge discovery, treating\\nLLMs as agents to first decompose tasks into multiple subtasks,\\nand then identifying the most suitable tools (e.g., GNNs) for\\neach subtask may potentially yield enhanced performance.\\nFurthermore, employing LLMs as agents holds promise for\\nconstructing a powerful and highly generalizable solver for\\ngraph-related tasks.\\n7 Conclusion\\nThe application of LLMs to graph-related tasks has emerged\\nas a prominent area of research in recent years. In this survey,\\nwe aim to provide an in-depth overview of existing strate-\\ngies for adapting LLMs to graphs. Firstly, we introduce a\\nnovel taxonomy that categorizes techniques involving both\\ngraph and text modalities into three categories based on the\\ndifferent roles played by LLMs, i.e., enhancer, predictor, and\\nalignment component. Secondly, we systematically review the\\nrepresentative studies according to the taxonomy. Finally, we\\ndiscuss some limitations and highlight several future research\\ndirections. Through this comprehensive review, we aspire to\\nshed light on the advancements and challenges in the field\\nof graph learning with LLMs, thereby encouraging further\\nenhancements in this domain.\\nReferences\\nRachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol\\nAhn. Can we trust the evaluation on chatgpt? arXiv preprint\\narXiv:2303.12767 , 2023.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e88bf7b-0eee-41ae-982e-5919ed15f678', embedding=None, metadata={'page_label': '10', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0d878a012a2de6718ced2f3f2547d56e74e929ba212b45d5ae2a879b3cb1772e', text='Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pre-\\ntrained language model for scientific text. arXiv preprint\\narXiv:1903.10676 , 2019.\\nAndres M Bran, Sam Cox, Andrew D White, and Philippe\\nSchwaller. Chemcrow: Augmenting large-language models\\nwith chemistry tools. arXiv preprint arXiv:2304.05376 ,\\n2023.\\nUlrik Brandes, Markus Eiglsperger, J ¨urgen Lerner, and Chris-\\ntian Pich. Graph markup language (graphml). 2013.\\nWilliam Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang,\\nBrandon Roy, Jad Kabbara, and Deb Roy. Congrat: Self-\\nsupervised contrastive pretraining for joint graph and text\\nembeddings. arXiv preprint arXiv:2305.14321 , 2023.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\\nguage models are few-shot learners. Advances in neural\\ninformation processing systems , 33:1877–1901, 2020.\\nZiwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai\\nHu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting\\ngraph reasoning ability of large language model. arXiv\\npreprint arXiv:2310.05845 , 2023.\\nShantanu Chandra, Pushkar Mishra, Helen Yannakoudakis,\\nMadhav Nimishakavi, Marzieh Saeidi, and Ekaterina\\nShutova. Graph-based modeling of online communities\\nfor fake news detection. arXiv preprint arXiv:2008.06274 ,\\n2020.\\nZhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen,\\nXiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan,\\nHui Liu, et al. Exploring the potential of large lan-\\nguage models (llms) in learning on graphs. arXiv preprint\\narXiv:2307.03393 , 2023.\\nZhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin,\\nHaiyang Zhang, Hui Liu, and Jiliang Tang. Label-free node\\nclassification on graphs with large language models (llms).\\narXiv preprint arXiv:2310.04668 , 2023.\\nJiashun Cheng, Man Li, Jia Li, and Fugee Tsung. Wiener\\ngraph deconvolutional network improves graph self-\\nsupervised learning. In Thirty-Seventh AAAI Conference on\\nArtificial Intelligence , pages 7131–7139, 2023.\\nEli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu,\\nJiong Zhang, Olgica Milenkovic, and Inderjit S Dhillon.\\nNode feature extraction by self-supervised multi-scale\\nneighborhood prediction. arXiv preprint arXiv:2111.00064 ,\\n2021.\\nIan Chiswell and Wilfrid Hodges. Mathematical logic . OUP\\nOxford, 2007.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\\nGehrmann, et al. Palm: Scaling language modeling with\\npathways. arXiv preprint arXiv:2204.02311 , 2022.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scaling\\ninstruction-finetuned language models. arXiv preprint\\narXiv:2210.11416 , 2022.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\\nWu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang\\nSui. A survey for in-context learning. arXiv preprint\\narXiv:2301.00234 , 2022.\\nKeyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan,\\nWei Tsang Ooi, Qizhe Xie, and Junxian He. Simteg: A\\nfrustratingly simple approach improves textual graph learn-\\ning. arXiv preprint arXiv:2308.02565 , 2023.\\nVijay Prakash Dwivedi, Ladislav Ramp ´aˇsek, Michael Galkin,\\nAli Parviz, Guy Wolf, Anh Tuan Luu, and Dominique\\nBeaini. Long range graph benchmark. Advances in Neural\\nInformation Processing Systems , 35:22326–22340, 2022.\\nCarl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol:\\nCross-modal molecule retrieval with natural language\\nqueries. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing , pages\\n595–607, 2021.\\nBahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk\\nlike a graph: Encoding graphs for large language models.\\narXiv preprint arXiv:2310.04560 , 2023.\\nJiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large\\nlanguage models understand graph structured data? an\\nempirical evaluation and benchmarking. arXiv preprint\\narXiv:2305.15066 , 2023.\\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive\\nrepresentation learning on large graphs. Advances in neural\\ninformation processing systems , 30, 2017.\\nJiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang. Ge-\\nometrically equivariant graph neural networks: A survey.\\narXiv preprint arXiv:2202.07230 , 2022.\\nXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong\\nZhang, and Meng Wang. Lightgcn: Simplifying and pow-\\nering graph convolution network for recommendation. In\\nProceedings of the 43rd International ACM SIGIR confer-\\nence on research and development in Information Retrieval ,\\npages 639–648, 2020.\\nXiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan\\nHooi. Explanations as features: Llm-based features for\\ntext-attributed graphs. arXiv preprint arXiv:2305.19523 ,\\n2023.\\nMichael Himsolt. Gml: Graph modelling language. University\\nof Passau , 1997.\\nZhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia\\nYang, Chunjie Wang, and Jie Tang. Graphmae: Self-\\nsupervised masked graph autoencoders. In Proceedings\\nof the 28th ACM SIGKDD Conference on Knowledge Dis-\\ncovery and Data Mining , pages 594–604, 2022.\\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong,\\nHongyu Ren, Bowen Liu, Michele Catasta, and Jure\\nLeskovec. Open graph benchmark: Datasets for machine\\nlearning on graphs. Advances in neural information pro-\\ncessing systems , 33:22118–22133, 2020.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b607b931-9824-4f67-bf87-f90414c60a96', embedding=None, metadata={'page_label': '11', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='34a1c9018b91fe1ec33d76ed31a205d46af0e7c99b9e8c28ff89e181e096d3a3', text='Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\\nShean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-\\nrank adaptation of large language models. In International\\nConference on Learning Representations , 2021.\\nYuntong Hu, Zheng Zhang, and Liang Zhao. Beyond text: A\\ndeep dive into large language models’ ability on understand-\\ning graph data. arXiv preprint arXiv:2310.04944 , 2023.\\nJin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. Can\\nllms effectively leverage graph structural information: when\\nand why. arXiv preprint arXiv:2309.16595 , 2023.\\nXuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao,\\nZhisheng Zhang, Yang Yang, and Qi Zhu. Prompt-\\nbased node feature extractor for few-shot learning on text-\\nattributed graphs. arXiv preprint arXiv:2309.02848 , 2023.\\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and\\nS Yu Philip. A survey on knowledge graphs: Representation,\\nacquisition, and applications. IEEE transactions on neural\\nnetworks and learning systems , 33(2):494–514, 2021.\\nJunguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng\\nLong. Transferability in deep learning: A survey. arXiv\\npreprint arXiv:2201.05867 , 2022.\\nBowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang\\nZhang, Qi Zhu, and Jiawei Han. Patton: Language\\nmodel pretraining on text-rich networks. arXiv preprint\\narXiv:2305.12268 , 2023.\\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\\nToutanova. Bert: Pre-training of deep bidirectional trans-\\nformers for language understanding. In Proceedings of\\nNAACL-HLT , pages 4171–4186, 2019.\\nThomas N Kipf and Max Welling. Semi-supervised classifi-\\ncation with graph convolutional networks. In International\\nConference on Learning Representations , 2016.\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing\\ncontinuous prompts for generation. In Proceedings of the\\n59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on\\nNatural Language Processing (Volume 1: Long Papers) ,\\npages 4582–4597, 2021.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\\n2: Bootstrapping language-image pre-training with frozen\\nimage encoders and large language models. arXiv preprint\\narXiv:2301.12597 , 2023.\\nYichuan Li, Kaize Ding, and Kyumin Lee. Grenade: Graph-\\ncentric language model for self-supervised representa-\\ntion learning on text-attributed graphs. arXiv preprint\\narXiv:2310.15109 , 2023.\\nChang Liu and Bo Wu. Evaluating large language models\\non graphs: Performance insights and comparative analysis.\\narXiv preprint arXiv:2308.11224 , 2023.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. Roberta: A ro-\\nbustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 , 2019.Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuo-\\nran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and An-\\nima Anandkumar. Multi-modal molecule structure-text\\nmodel for text-based retrieval and editing. arXiv preprint\\narXiv:2212.10789 , 2022.\\nHao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng\\nTao, Yixin Chen, and Muhan Zhang. One for all: Towards\\ntraining one graph model for all classification tasks. arXiv\\npreprint arXiv:2310.00149 , 2023.\\nJiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li,\\nMengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S\\nYu, et al. Towards graph foundation models: A survey and\\nbeyond. arXiv preprint arXiv:2310.11829 , 2023.\\nPengfei Liu, Yiming Ren, and Zhixiang Ren. Git-mol: A multi-\\nmodal large language model for molecular science with\\ngraph, image, and text. arXiv preprint arXiv:2308.06911 ,\\n2023.\\nZhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin\\nCao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua.\\nMolca: Molecular graph-language modeling with cross-\\nmodal projector and uni-modal adapter. arXiv preprint\\narXiv:2310.12798 , 2023.\\nCostas Mavromatis, Vassilis N Ioannidis, Shen Wang,\\nDa Zheng, Soji Adeshina, Jun Ma, Han Zhao, Christos\\nFaloutsos, and George Karypis. Train your own gnn teacher:\\nGraph-aware distillation on textual graphs. arXiv preprint\\narXiv:2304.10668 , 2023.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,\\nand Jeff Dean. Distributed representations of words and\\nphrases and their compositionality. Advances in neural\\ninformation processing systems , 26, 2013.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Represen-\\ntation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748 , 2018.\\nOpenAI. Gpt-4 technical report, 2023.\\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-\\nith Ringel Morris, Percy Liang, and Michael S Bernstein.\\nGenerative agents: Interactive simulacra of human behavior.\\narXiv preprint arXiv:2304.03442 , 2023.\\nChen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong\\nLiu. Can large language models empower molecular prop-\\nerty prediction? arXiv preprint arXiv:2307.07443 , 2023.\\nYijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Disen-\\ntangled representation learning with large language models\\nfor text-attributed graphs. arXiv preprint arXiv:2310.18152 ,\\n2023.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, et al. Learning trans-\\nferable visual models from natural language supervision.\\nInInternational conference on machine learning , pages\\n8748–8763. PMLR, 2021.\\nJoshua Robinson and David Wingate. Leveraging large lan-\\nguage models for multiple choice question answering. In', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d4a1684-27fb-4bf1-942f-957b231a58f3', embedding=None, metadata={'page_label': '12', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='af54baf1a2bafb4919ebc47a18a52fdb7e933ce77aa4666a6d1440bd5ed0d68b', text='The Eleventh International Conference on Learning Repre-\\nsentations , 2022.\\nGerard Salton and Christopher Buckley. Term-weighting ap-\\nproaches in automatic text retrieval. Information processing\\n& management , 24(5):513–523, 1988.\\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,\\nBrian Galligher, and Tina Eliassi-Rad. Collective classifica-\\ntion in network data. AI magazine , 29(3):93–93, 2008.\\nErfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram\\nZaree, Yue Dong, and Nael Abu-Ghazaleh. Survey of vul-\\nnerabilities in large language models revealed by adversarial\\nattacks. arXiv preprint arXiv:2310.10844 , 2023.\\nYaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, and\\nXiang Wang. Relm: Leveraging language models for\\nenhanced chemical reaction prediction. arXiv preprint\\narXiv:2310.13590 , 2023.\\nChao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan.\\nSpatial-temporal synchronous graph convolutional net-\\nworks: A new framework for spatial-temporal network data\\nforecasting. In Proceedings of the AAAI conference on\\nartificial intelligence , volume 34, pages 914–921, 2020.\\nBing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng\\nLi, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen.\\nA molecular multimodal foundation model associating\\nmolecule graphs with natural language. arXiv preprint\\narXiv:2209.05481 , 2022.\\nMingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and\\nXin Wang. Gppt: Graph pre-training and prompt tuning to\\ngeneralize graph neural networks. In Proceedings of the\\n28th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining , pages 1717–1727, 2022.\\nXiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan.\\nAll in one: Multi-task prompting for graph neural networks.\\n2023.\\nXiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen,\\nGuandong Xu, and Hongzhi Yin. Self-supervised hy-\\npergraph representation learning for sociological analysis.\\nIEEE Transactions on Knowledge and Data Engineering ,\\n2023.\\nQiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li,\\nRui Chen, and Xia Hu. S2gae: Self-supervised graph au-\\ntoencoders are generalizable learners with graph masking.\\nInProceedings of the Sixteenth ACM International Con-\\nference on Web Search and Data Mining , pages 787–795,\\n2023.\\nYanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, and Carl\\nYang. Walklm: A uniform language model fine-tuning\\nframework for attributed graph embedding. In Thirty-\\nseventh Conference on Neural Information Processing Sys-\\ntems, 2023.\\nJiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi\\nCheng, Dawei Yin, and Chao Huang. Graphgpt: Graph\\ninstruction tuning for large language models. arXiv preprint\\narXiv:2310.13023 , 2023.Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas\\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-\\nton, Viktor Kerkez, and Robert Stojnic. Galactica:\\nA large language model for science. arXiv preprint\\narXiv:2211.09085 , 2022.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\\ntinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste\\nRozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\\nLlama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\\nLlion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017.\\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adri-\\nana Romero, Pietro Lio, and Yoshua Bengio. Graph atten-\\ntion networks. stat, 1050:4, 2018.\\nYanli Wang, Jewen Xiao, Tugba O Suzek, Jian Zhang, Jiyao\\nWang, and Stephen H Bryant. Pubchem: a public informa-\\ntion system for analyzing bioactivities of small molecules.\\nNucleic acids research , 37(suppl 2):W623–W633, 2009.\\nHaishuai Wang, Yang Gao, Xin Zheng, Peng Zhang,\\nHongyang Chen, and Jiajun Bu. Graph neural architec-\\nture search with gpt-4. arXiv preprint arXiv:2310.01436 ,\\n2023.\\nHeng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan,\\nXiaochuang Han, and Yulia Tsvetkov. Can language models\\nsolve graph problems in natural language? arXiv preprint\\narXiv:2305.10037 , 2023.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-\\nret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\\nBosma, Denny Zhou, Donald Metzler, et al. Emergent abil-\\nities of large language models. Transactions on Machine\\nLearning Research , 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,\\nFei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-\\nof-thought prompting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Sys-\\ntems, 35:24824–24837, 2022.\\nWei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su,\\nSuqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang.\\nLlmrec: Large language models with graph augmentation\\nfor recommendation. arXiv preprint arXiv:2311.00423 ,\\n2023.\\nZhihao Wen and Yuan Fang. Prompt tuning on graph-\\naugmented low-resource text classification. arXiv preprint\\narXiv:2307.10230 , 2023.\\nLilian Weng. Llm-powered autonomous agents. lilian-\\nweng.github.io , Jun 2023.\\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph\\nGomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing,\\nand Vijay Pande. Moleculenet: a benchmark for molecular\\nmachine learning. Chemical science , 9(2):513–530, 2018.\\nHan Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N Ioanni-\\ndis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yi Xu,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93a2b556-532a-4e9d-bfb3-ea9e98011238', embedding=None, metadata={'page_label': '13', 'file_name': '25b4824683c0f3870d9744973d6258bd.pdf', 'Title of this paper': 'A Survey of Graph Meets Large Language Model: Progress and Future Directions', 'Authors': 'Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu', 'Date published': '11/21/2023', 'URL': 'http://arxiv.org/abs/2311.12399v2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2a10dc8fd47c4d019905cc5634a16351e45f7ca7a5615edb435c4f837405c21f', text='et al. Graph-aware language model pre-training on a large\\ngraph corpus can help multiple graph applications. arXiv\\npreprint arXiv:2306.02592 , 2023.\\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\\nHow powerful are graph neural networks? arXiv preprint\\narXiv:1810.00826 , 2018.\\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revis-\\niting semi-supervised learning with graph embeddings. In\\nInternational conference on machine learning , pages 40–48.\\nPMLR, 2016.\\nJunhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu\\nLian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, and\\nXing Xie. Graphformers: Gnn-nested transformers for rep-\\nresentation learning on textual graph. Advances in Neural\\nInformation Processing Systems , 34:28798–28810, 2021.\\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,\\nQizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Har-\\nnessing the power of llms in practice: A survey on chatgpt\\nand beyond. arXiv preprint arXiv:2304.13712 , 2023.\\nRuosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and\\nYongfeng Zhang. Natural language is all a graph needs.\\narXiv preprint arXiv:2308.07134 , 2023.\\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng,\\nGuolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do\\ntransformers really perform badly for graph representa-\\ntion? Advances in Neural Information Processing Systems ,\\n34:28877–28888, 2021.\\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen,\\nZhangyang Wang, and Yang Shen. Graph contrastive learn-\\ning with augmentations. Advances in neural information\\nprocessing systems , 33:5812–5823, 2020.\\nJianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang\\nLi, and Xuecang Zhang. Empower text-attributed graphs\\nlearning with large language models (llms). arXiv preprint\\narXiv:2310.09872 , 2023.\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\\nXiao Xia, et al. Glm-130b: An open bilingual pre-trained\\nmodel. arXiv preprint arXiv:2210.02414 , 2022.\\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An\\ninstruction-tuned audio-visual language model for video\\nunderstanding. arXiv preprint arXiv:2306.02858 , 2023.\\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin\\nYan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-\\nadapter: Efficient fine-tuning of language models with zero-\\ninit attention. arXiv preprint arXiv:2303.16199 , 2023.\\nZeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian\\nQin, Simin Wu, and Wenwu Zhu. Llm4dyg: Can large\\nlanguage models solve problems on dynamic graphs? arXiv\\npreprint arXiv:2310.17110 , 2023.\\nZiwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin\\nWang, and Wenwu Zhu. Large graph models: A perspective.\\narXiv preprint arXiv:2308.14522 , 2023.Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui\\nLi, Xing Xie, and Jian Tang. Learning on large-scale text-\\nattributed graphs via variational inference. arXiv preprint\\narXiv:2210.14709 , 2022.\\nHaiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie\\nFu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. Gimlet:\\nA unified graph-text model for instruction-based molecule\\nzero-shot learning. arXiv preprint arXiv:2306.13089 , 2023.\\nHaiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi\\nDeng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and\\nMengnan Du. Explainability for large language models:\\nA survey. arXiv preprint arXiv:2309.01029 , 2023.\\nJianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu,\\nMichael Bronstein, Zhaocheng Zhu, and Jian Tang. Graph-\\ntext: Graph reasoning in text space. arXiv preprint\\narXiv:2310.01089 , 2023.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei\\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. A survey of large language\\nmodels. arXiv preprint arXiv:2303.18223 , 2023.\\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and\\nLiang Wang. Graph contrastive learning with adaptive aug-\\nmentation. In Proceedings of the Web Conference 2021 ,\\npages 2069–2080, 2021.\\nJing Zhu, Xiang Song, Vassilis N Ioannidis, Danai Koutra, and\\nChristos Faloutsos. Touchup-g: Improving feature repre-\\nsentation through graph-centric finetuning. arXiv preprint\\narXiv:2309.13885 , 2023.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76d12cf2-0560-4da4-a1e5-3ad2867d83a2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='be39898ab7d8bfb999a52fd82b1e1a6786f2885810557ebf61e407b6f821c91b', text='The following is a summary of the paper: A Survey of Graph Meets Large Language Model: Progress and Future Directions\\n\\nSummary: Graph plays a significant role in representing and analyzing complex\\nrelationships in real-world applications such as citation networks, social\\nnetworks, and biological data. Recently, Large Language Models (LLMs), which\\nhave achieved tremendous success in various domains, have also been leveraged\\nin graph-related tasks to surpass traditional Graph Neural Networks (GNNs)\\nbased methods and yield state-of-the-art performance. In this survey, we first\\npresent a comprehensive review and analysis of existing methods that integrate\\nLLMs with graphs. First of all, we propose a new taxonomy, which organizes\\nexisting methods into three categories based on the role (i.e., enhancer,\\npredictor, and alignment component) played by LLMs in graph-related tasks. Then\\nwe systematically survey the representative methods along the three categories\\nof the taxonomy. Finally, we discuss the remaining limitations of existing\\nstudies and highlight promising avenues for future research. The relevant\\npapers are summarized and will be consistently updated at:\\nhttps://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import download_loader\n",
    "\n",
    "# ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "# loader = ArxivReader()\n",
    "# documents, abstracts = loader.load_papers_and_abstracts(search_query='id:2311.12399', max_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='a9fd4f92-de35-49f9-8b1a-71f9541814d6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='be39898ab7d8bfb999a52fd82b1e1a6786f2885810557ebf61e407b6f821c91b', text='The following is a summary of the paper: A Survey of Graph Meets Large Language Model: Progress and Future Directions\\n\\nSummary: Graph plays a significant role in representing and analyzing complex\\nrelationships in real-world applications such as citation networks, social\\nnetworks, and biological data. Recently, Large Language Models (LLMs), which\\nhave achieved tremendous success in various domains, have also been leveraged\\nin graph-related tasks to surpass traditional Graph Neural Networks (GNNs)\\nbased methods and yield state-of-the-art performance. In this survey, we first\\npresent a comprehensive review and analysis of existing methods that integrate\\nLLMs with graphs. First of all, we propose a new taxonomy, which organizes\\nexisting methods into three categories based on the role (i.e., enhancer,\\npredictor, and alignment component) played by LLMs in graph-related tasks. Then\\nwe systematically survey the representative methods along the three categories\\nof the taxonomy. Finally, we discuss the remaining limitations of existing\\nstudies and highlight promising avenues for future research. The relevant\\npapers are summarized and will be consistently updated at:\\nhttps://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
