{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms import Ollama\n",
    "import numpy as np\n",
    "from trulens_eval import TruLlama, Feedback, Tru, feedback\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "tru = Tru()\n",
    "\n",
    "load_dotenv('/Users/jeana/.env')\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.WARNING\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_location = [r\"/Users/jeana/Retrieval-Augmented-Generation/LlamaIndex/paul_graham_essay.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelException(Exception):\n",
    "    def __init__(self, invalid_value, allowed_values):\n",
    "        self.invalid_value = invalid_value\n",
    "        self.allowed_values = allowed_values\n",
    "        message = f\"Invalid value: {invalid_value}. Allowed values are: {', '.join(allowed_values)}\"\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluator with Feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedbacks():\n",
    "\n",
    "    ##### INITIALZE FEEDBACK FUNCTION(S)#######\n",
    "\n",
    "    # Initialize AzureOpenAI-based feedback function collection class:\n",
    "    azopenai = feedback.AzureOpenAI(\n",
    "                                    deployment_name=os.environ['OPENAI_DEPLOYMENT_NAME'],\n",
    "                                    api_key = os.environ['OPENAI_API_KEY'],\n",
    "                                    api_version=os.environ['OPENAI_DEPLOYMENT_VERSION'],\n",
    "                                    azure_endpoint=os.environ['OPENAI_DEPLOYMENT_ENDPOINT'],\n",
    "                                    # model = os.environ['OPENAI_MODEL_NAME']\n",
    "                                    )\n",
    "\n",
    "    # Question/answer relevance between overall question and answer.\n",
    "    f_qa_relevance = Feedback(azopenai.relevance, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "    # Question/statement relevance between question and each context chunk.\n",
    "    f_qs_relevance = Feedback(azopenai.qs_relevance, name = \"Context Relevance\").on_input().on(\n",
    "        TruLlama.select_source_nodes().node.text\n",
    "    ).aggregate(np.mean)\n",
    "\n",
    "    # groundedness of output on the context\n",
    "    groundedness = feedback.Groundedness(\n",
    "                    # summarize_provider=azopenai, \n",
    "                    groundedness_provider=azopenai)\n",
    "    f_groundedness = Feedback(groundedness.groundedness_measure, name = \"Groundedness\").on(TruLlama.select_source_nodes().node.text).on_output()\n",
    "    \n",
    "    feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance]\n",
    "    \n",
    "    return feedbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:trulens_eval.feedback.provider.endpoint.openai:Arguments ['api_key', 'api_version', 'azure_endpoint'] are ignored as `client` was provided.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments ['api_key', 'api_version', 'azure_endpoint'] are ignored as `client` was provided.\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "feedbacks = feedbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG():\n",
    "    llm_list = ['llama2', 'gpt3.5Turbo']\n",
    "    embedding_list = ['text-embedding-ada-002', 'sentence-transformers/all-mpnet-base-v2', \"BAAI/bge-small-en-v1.5\"]\n",
    "\n",
    "    def __init__(self, llm_name, embedding_model, feedbacks):   \n",
    "        if llm_name not in self.llm_list:\n",
    "            raise modelException(llm_name, self.llm_list)\n",
    "        if embedding_model not in self.embedding_list:\n",
    "            raise modelException(embedding_model, self.embedding_list)        \n",
    "\n",
    "        ## Getting the LLM for prediction\n",
    "        if llm_name == 'gpt3.5Turbo':\n",
    "            self.llm = AzureOpenAI(\n",
    "                    # model= os.environ['OPENAI_MODEL_NAME'],\n",
    "                    model = llm_name,\n",
    "                    deployment_name= os.environ['OPENAI_DEPLOYMENT_NAME'],\n",
    "                    api_key=os.environ['OPENAI_API_KEY'],\n",
    "                    azure_endpoint=os.environ['OPENAI_DEPLOYMENT_ENDPOINT'],\n",
    "                    api_version=os.environ['OPENAI_DEPLOYMENT_VERSION'],\n",
    "                )\n",
    "        elif llm_name == 'llama2':\n",
    "            self.llm = Ollama(model=\"llama2\")\n",
    "        \n",
    "        ## Gettting the embedding model\n",
    "        if embedding_model == 'text-embedding-ada-002':\n",
    "            self.embedding_model = AzureOpenAIEmbedding(\n",
    "                    # model=os.environ['OPENAI_EMBEDDING_MODEL_NAME'],\n",
    "                    model=embedding_model,\n",
    "                    deployment_name=os.environ['OPENAI_EMBEDDING_DEPLOYMENT_NAME'],\n",
    "                    api_key=os.environ['OPENAI_API_KEY'],\n",
    "                    azure_endpoint=os.environ['OPENAI_DEPLOYMENT_ENDPOINT'],\n",
    "                    api_version=os.environ['OPENAI_DEPLOYMENT_VERSION'],\n",
    "                )\n",
    "        elif embedding_model in ['sentence-transformers/all-mpnet-base-v2', \"BAAI/bge-small-en-v1.5\"]:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                    model_name=embedding_model)\n",
    "        \n",
    "        #Set service context, documents and Index\n",
    "        self.service_context = ServiceContext.from_defaults(embed_model=self.embedding_model, llm = self.llm)\n",
    "\n",
    "        self.documents = SimpleDirectoryReader(\n",
    "                    input_files=[r\"/Users/jeana/Retrieval-Augmented-Generation/LlamaIndex/paul_graham_essay.txt\"] #or just indicate the fullpath of the folder containing the data\n",
    "                                ).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents, service_context=self.service_context)\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        ### INSTRUMENT CHAIN FOR LOGGING WITH TRULENS\n",
    "\n",
    "        query_engine = self.index.as_query_engine()\n",
    "\n",
    "        tru_query_engine_recorder = TruLlama(query_engine,\n",
    "                app_id='LlamaIndex_App1',\n",
    "                feedbacks=feedbacks)\n",
    "\n",
    "        with tru_query_engine_recorder as recorder:\n",
    "            answer = query_engine.query(query)\n",
    "            print(answer.get_formatted_sources())\n",
    "            print(\"query was:\", query)\n",
    "            print(\"answer was:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name =  'llama2' #gpt3.5Turbo , llama2\n",
    "embedding_model = 'sentence-transformers/all-mpnet-base-v2' # text-embedding-ada-002 , sentence-transformers/all-mpnet-base-v2, local\n",
    "rag = RAG(llm_name, embedding_model, feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAG at 0x167b80890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
